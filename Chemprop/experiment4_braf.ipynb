{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6192b6bb-dd6f-43c8-9752-05f9de1700b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aih/serra.korkmaz/miniconda3/envs/saturn/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-11 22:35:47,171\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-03-11 22:35:47,516\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-03-11 22:35:48,092\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from lightning import pytorch as pl\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.train import CheckpointConfig, RunConfig, ScalingConfig\n",
    "from ray.train.lightning import (RayDDPStrategy, RayLightningEnvironment,\n",
    "                                 RayTrainReportCallback, prepare_trainer)\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.schedulers import FIFOScheduler\n",
    "\n",
    "from chemprop import data, featurizers, models, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afed6433-23c1-499c-9c56-03c725f26577",
   "metadata": {},
   "outputs": [],
   "source": [
    "chemprop_dir = Path.cwd().parent\n",
    "train_path = \"/home/aih/serra.korkmaz/projects/saturn/SurogateModel/training_data_4sm_chembl/train_data_braf.csv\"\n",
    "test_path = \"/home/aih/serra.korkmaz/projects/saturn/SurogateModel/training_data_4sm_chembl/test_data_braf.csv\"\n",
    "num_workers = 0 # number of workers for dataloader. 0 means using main process for data loading\n",
    "smiles_column = 'smiles' # name of the column containing SMILES strings\n",
    "target_columns = ['value'] # list of names of the columns containing targets\n",
    "\n",
    "hpopt_save_dir = Path.cwd() / \"hpopt_braf_chembl\" # directory to save hyperopt results\n",
    "hpopt_save_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79b9963b-a4e2-4cd8-b122-d379814088b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c1c3921-b3da-4341-8140-74cf0ecafad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract SMILES and target values\n",
    "train_smis = df_train[smiles_column].values\n",
    "train_ys = df_train[target_columns].values\n",
    "\n",
    "test_smis = df_test[smiles_column].values\n",
    "test_ys = df_test[target_columns].values\n",
    "\n",
    "# Convert data to MoleculeDatapoint format\n",
    "train_data = [data.MoleculeDatapoint.from_smi(smi, y) for smi, y in zip(train_smis, train_ys)]\n",
    "test_data = [data.MoleculeDatapoint.from_smi(smi, y) for smi, y in zip(test_smis, test_ys)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbc5e27a-cfbc-4181-89d3-83b27785fcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize featurizer\n",
    "featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
    "\n",
    "# Create training dataset\n",
    "train_dset = data.MoleculeDataset(train_data, featurizer)\n",
    "scaler = train_dset.normalize_targets()\n",
    "\n",
    "# Split validation set from training data (80% train, 20% validation)\n",
    "split_idx = int(len(train_data) * 0.8)\n",
    "val_data = train_data[split_idx:]\n",
    "train_data = train_data[:split_idx]\n",
    "\n",
    "# Create validation and test datasets\n",
    "val_dset = data.MoleculeDataset(val_data, featurizer)\n",
    "val_dset.normalize_targets(scaler)\n",
    "\n",
    "test_dset = data.MoleculeDataset(test_data, featurizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0571af4-13e8-4c32-add1-bc9dd10df31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, train_dset, val_dset, num_workers, scaler):\n",
    "\n",
    "    # config is a dictionary containing hyperparameters used for the trial\n",
    "    depth = int(config[\"depth\"])\n",
    "    ffn_hidden_dim = int(config[\"ffn_hidden_dim\"])\n",
    "    ffn_num_layers = int(config[\"ffn_num_layers\"])\n",
    "    message_hidden_dim = int(config[\"message_hidden_dim\"])\n",
    "\n",
    "    train_loader = data.build_dataloader(train_dset, num_workers=num_workers, shuffle=True)\n",
    "    val_loader = data.build_dataloader(val_dset, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "    mp = nn.BondMessagePassing(d_h=message_hidden_dim, depth=depth)\n",
    "    agg = nn.MeanAggregation()\n",
    "    output_transform = nn.UnscaleTransform.from_standard_scaler(scaler)\n",
    "    ffn = nn.RegressionFFN(output_transform=output_transform, input_dim=message_hidden_dim, hidden_dim=ffn_hidden_dim, n_layers=ffn_num_layers)\n",
    "    batch_norm = True\n",
    "    metric_list = [nn.metrics.RMSE(), nn.metrics.MAE()]\n",
    "    model = models.MPNN(mp, agg, ffn, batch_norm, metric_list)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        max_epochs=20, # number of epochs to train for\n",
    "        # below are needed for Ray and Lightning integration\n",
    "        strategy=RayDDPStrategy(),\n",
    "        callbacks=[RayTrainReportCallback()],\n",
    "        plugins=[RayLightningEnvironment()],\n",
    "    )\n",
    "\n",
    "    trainer = prepare_trainer(trainer)\n",
    "    trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00dc82d6-6d1d-489c-802a-ae0a223867dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"depth\": tune.qrandint(lower=2, upper=6, q=1),\n",
    "    \"ffn_hidden_dim\": tune.qrandint(lower=300, upper=2400, q=100),\n",
    "    \"ffn_num_layers\": tune.qrandint(lower=1, upper=3, q=1),\n",
    "    \"message_hidden_dim\": tune.qrandint(lower=300, upper=2400, q=100),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c406b1c1-394a-43f0-be69-3fe9b9d593a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-03-11 22:41:25</td></tr>\n",
       "<tr><td>Running for: </td><td>00:04:31.64        </td></tr>\n",
       "<tr><td>Memory:      </td><td>88.2/753.9 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 2.0/96 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc              </th><th style=\"text-align: right;\">  train_loop_config/de\n",
       "pth</th><th style=\"text-align: right;\">     train_loop_config/ff\n",
       "n_hidden_dim</th><th style=\"text-align: right;\">  train_loop_config/ff\n",
       "n_num_layers</th><th style=\"text-align: right;\">    train_loop_config/me\n",
       "ssage_hidden_dim</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  train_loss_step</th><th style=\"text-align: right;\">  val/rmse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_60537ef6</td><td>TERMINATED</td><td>10.233.0.37:60246</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">500</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         260.43 </td><td style=\"text-align: right;\">    0.239256</td><td style=\"text-align: right;\">         0.158691</td><td style=\"text-align: right;\">  0.442863</td></tr>\n",
       "<tr><td>TorchTrainer_73171dbb</td><td>TERMINATED</td><td>10.233.0.37:60487</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">2200</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">400</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         232.149</td><td style=\"text-align: right;\">    0.217354</td><td style=\"text-align: right;\">         0.119868</td><td style=\"text-align: right;\">  0.425128</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TorchTrainer pid=60246)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=60246)\u001b[0m - (node_id=f02633f26ddf7d7c44cd961bf4e4ec1c58cf2a4fe3ef7b1bcdcea048, ip=10.233.0.37, pid=60488) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m /home/aih/serra.korkmaz/miniconda3/envs/saturn/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m /home/aih/serra.korkmaz/miniconda3/envs/saturn/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m /home/aih/serra.korkmaz/miniconda3/envs/saturn/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (29) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m   | Name            | Type               | Params | Mode \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m ---------------------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m 0 | message_passing | BondMessagePassing | 579 K  | train\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m 1 | agg             | MeanAggregation    | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m 2 | bn              | BatchNorm1d        | 1.0 K  | train\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m 3 | predictor       | RegressionFFN      | 5.0 M  | train\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m 4 | X_d_transform   | Identity           | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m 5 | metrics         | ModuleList         | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m ---------------------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m 5.6 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m 5.6 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m 22.346    Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m 27        Modules in train mode\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m /home/aih/serra.korkmaz/miniconda3/envs/saturn/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m /home/aih/serra.korkmaz/miniconda3/envs/saturn/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  3.19it/s]\n",
      "Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s]                            \n",
      "Epoch 0:   3%|▎         | 1/29 [00:00<00:11,  2.39it/s, v_num=3.41e+7, train_loss_step=0.791]\n",
      "Epoch 0:   7%|▋         | 2/29 [00:00<00:10,  2.57it/s, v_num=3.41e+7, train_loss_step=0.772]\n",
      "Epoch 0:  10%|█         | 3/29 [00:01<00:09,  2.64it/s, v_num=3.41e+7, train_loss_step=0.576]\n",
      "Epoch 0:  14%|█▍        | 4/29 [00:01<00:09,  2.72it/s, v_num=3.41e+7, train_loss_step=0.612]\n",
      "Epoch 0:  17%|█▋        | 5/29 [00:01<00:08,  2.74it/s, v_num=3.41e+7, train_loss_step=0.670]\n",
      "Epoch 0:  21%|██        | 6/29 [00:02<00:08,  2.78it/s, v_num=3.41e+7, train_loss_step=0.712]\n",
      "Epoch 0:  24%|██▍       | 7/29 [00:02<00:07,  2.83it/s, v_num=3.41e+7, train_loss_step=0.483]\n",
      "Epoch 0:  28%|██▊       | 8/29 [00:02<00:07,  2.84it/s, v_num=3.41e+7, train_loss_step=0.649]\n",
      "Epoch 0:  31%|███       | 9/29 [00:03<00:06,  2.86it/s, v_num=3.41e+7, train_loss_step=0.673]\n",
      "Epoch 0:  34%|███▍      | 10/29 [00:03<00:06,  2.87it/s, v_num=3.41e+7, train_loss_step=0.485]\n",
      "Epoch 0:  38%|███▊      | 11/29 [00:03<00:06,  2.86it/s, v_num=3.41e+7, train_loss_step=0.451]\n",
      "Epoch 0:  41%|████▏     | 12/29 [00:04<00:06,  2.83it/s, v_num=3.41e+7, train_loss_step=0.462]\n",
      "Epoch 0:  45%|████▍     | 13/29 [00:04<00:05,  2.81it/s, v_num=3.41e+7, train_loss_step=0.880]\n",
      "Epoch 0:  48%|████▊     | 14/29 [00:04<00:05,  2.81it/s, v_num=3.41e+7, train_loss_step=0.933]\n",
      "Epoch 0:  52%|█████▏    | 15/29 [00:05<00:05,  2.79it/s, v_num=3.41e+7, train_loss_step=0.471]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m - (node_id=f02633f26ddf7d7c44cd961bf4e4ec1c58cf2a4fe3ef7b1bcdcea048, ip=10.233.0.37, pid=60724) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  55%|█████▌    | 16/29 [00:05<00:04,  2.80it/s, v_num=3.41e+7, train_loss_step=0.481]\n",
      "Epoch 0:  59%|█████▊    | 17/29 [00:06<00:04,  2.79it/s, v_num=3.41e+7, train_loss_step=0.385]\n",
      "Epoch 0:  62%|██████▏   | 18/29 [00:06<00:03,  2.80it/s, v_num=3.41e+7, train_loss_step=0.368]\n",
      "Epoch 0:  66%|██████▌   | 19/29 [00:06<00:03,  2.78it/s, v_num=3.41e+7, train_loss_step=0.578]\n",
      "Epoch 0:  69%|██████▉   | 20/29 [00:07<00:03,  2.78it/s, v_num=3.41e+7, train_loss_step=0.471]\n",
      "Epoch 0:  72%|███████▏  | 21/29 [00:07<00:02,  2.78it/s, v_num=3.41e+7, train_loss_step=0.522]\n",
      "Epoch 0:  76%|███████▌  | 22/29 [00:07<00:02,  2.78it/s, v_num=3.41e+7, train_loss_step=0.333]\n",
      "Epoch 0:  79%|███████▉  | 23/29 [00:08<00:02,  2.78it/s, v_num=3.41e+7, train_loss_step=0.592]\n",
      "Epoch 0:  83%|████████▎ | 24/29 [00:08<00:01,  2.79it/s, v_num=3.41e+7, train_loss_step=0.401]\n",
      "Epoch 0:  86%|████████▌ | 25/29 [00:08<00:01,  2.80it/s, v_num=3.41e+7, train_loss_step=0.721]\n",
      "Epoch 0:  90%|████████▉ | 26/29 [00:09<00:01,  2.80it/s, v_num=3.41e+7, train_loss_step=0.514]\n",
      "Epoch 0:  93%|█████████▎| 27/29 [00:09<00:00,  2.80it/s, v_num=3.41e+7, train_loss_step=0.476]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m /home/aih/serra.korkmaz/miniconda3/envs/saturn/lib/python3.11/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m GPU available: False, used: False\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m /home/aih/serra.korkmaz/miniconda3/envs/saturn/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m /home/aih/serra.korkmaz/miniconda3/envs/saturn/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (29) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m   | Name            | Type               | Params | Mode \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m ---------------------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m 0 | message_passing | BondMessagePassing | 383 K  | train\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m 1 | agg             | MeanAggregation    | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m 2 | bn              | BatchNorm1d        | 800    | train\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m 3 | predictor       | RegressionFFN      | 5.7 M  | train\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m 4 | X_d_transform   | Identity           | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m 5 | metrics         | ModuleList         | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m ---------------------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m 6.1 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m 6.1 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m 24.444    Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m 27        Modules in train mode\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m /home/aih/serra.korkmaz/miniconda3/envs/saturn/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m /home/aih/serra.korkmaz/miniconda3/envs/saturn/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  97%|█████████▋| 28/29 [00:09<00:00,  2.80it/s, v_num=3.41e+7, train_loss_step=0.623]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 0: 100%|██████████| 29/29 [00:10<00:00,  2.87it/s, v_num=3.41e+7, train_loss_step=1.190]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  3.93it/s]\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:01,  3.53it/s]\u001b[A\n",
      "Epoch 0:   0%|          | 0/29 [00:00<?, ?it/s]                            \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:01,  3.17it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00,  3.14it/s]\u001b[A\n",
      "Epoch 0:   3%|▎         | 1/29 [00:00<00:11,  2.45it/s, v_num=3.41e+7, train_loss_step=0.820]\n",
      "Epoch 0:   7%|▋         | 2/29 [00:00<00:10,  2.63it/s, v_num=3.41e+7, train_loss_step=0.758]\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:01<00:00,  3.26it/s]\u001b[A\n",
      "Epoch 0:  10%|█         | 3/29 [00:01<00:09,  2.63it/s, v_num=3.41e+7, train_loss_step=0.569]\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:01<00:00,  3.15it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 22:37:25,451\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  3.30it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 29/29 [00:11<00:00,  2.42it/s, v_num=3.41e+7, train_loss_step=1.190, val_loss=0.961]\n",
      "Epoch 0:  14%|█▍        | 4/29 [00:01<00:09,  2.67it/s, v_num=3.41e+7, train_loss_step=0.626]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/60537ef6/checkpoint_000000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 29/29 [00:12<00:00,  2.38it/s, v_num=3.41e+7, train_loss_step=1.190, val_loss=0.961, train_loss_epoch=0.577]\n",
      "Epoch 1:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=1.190, val_loss=0.961, train_loss_epoch=0.577]         \n",
      "Epoch 0:  17%|█▋        | 5/29 [00:01<00:08,  2.80it/s, v_num=3.41e+7, train_loss_step=0.664]\n",
      "Epoch 0:  21%|██        | 6/29 [00:02<00:08,  2.79it/s, v_num=3.41e+7, train_loss_step=0.733]\n",
      "Epoch 0:  45%|████▍     | 13/29 [00:04<00:05,  2.89it/s, v_num=3.41e+7, train_loss_step=0.464]\n",
      "Epoch 0:  45%|████▍     | 13/29 [00:04<00:05,  2.89it/s, v_num=3.41e+7, train_loss_step=0.822]\n",
      "Epoch 1:  34%|███▍      | 10/29 [00:04<00:07,  2.47it/s, v_num=3.41e+7, train_loss_step=0.505, val_loss=0.961, train_loss_epoch=0.577]\u001b[32m [repeated 19x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "Epoch 0:  93%|█████████▎| 27/29 [00:08<00:00,  3.14it/s, v_num=3.41e+7, train_loss_step=0.445]\n",
      "Epoch 0:  97%|█████████▋| 28/29 [00:08<00:00,  3.16it/s, v_num=3.41e+7, train_loss_step=0.680]\n",
      "Epoch 0: 100%|██████████| 29/29 [00:08<00:00,  3.24it/s, v_num=3.41e+7, train_loss_step=1.120]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00,  6.32it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00,  5.58it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00,  5.44it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00<00:00,  5.43it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00,  5.31it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  5.59it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 29/29 [00:10<00:00,  2.88it/s, v_num=3.41e+7, train_loss_step=1.120, val_loss=0.946]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/73171dbb/checkpoint_000000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 29/29 [00:10<00:00,  2.82it/s, v_num=3.41e+7, train_loss_step=1.120, val_loss=0.946, train_loss_epoch=0.576]\n",
      "Epoch 1:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=1.120, val_loss=0.946, train_loss_epoch=0.576]         \n",
      "Epoch 1:  86%|████████▌ | 25/29 [00:09<00:01,  2.77it/s, v_num=3.41e+7, train_loss_step=0.478, val_loss=0.961, train_loss_epoch=0.577]\u001b[32m [repeated 26x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 22:37:37,242\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/60537ef6/checkpoint_000001)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 29/29 [00:10<00:00,  2.87it/s, v_num=3.41e+7, train_loss_step=0.529, val_loss=0.961, train_loss_epoch=0.577]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:01<00:00,  4.30it/s]\u001b[A\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  4.54it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 29/29 [00:11<00:00,  2.53it/s, v_num=3.41e+7, train_loss_step=0.529, val_loss=0.909, train_loss_epoch=0.577]\n",
      "Epoch 1: 100%|██████████| 29/29 [00:11<00:00,  2.49it/s, v_num=3.41e+7, train_loss_step=0.529, val_loss=0.909, train_loss_epoch=0.518]\n",
      "Epoch 2:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.529, val_loss=0.909, train_loss_epoch=0.518]         \n",
      "Epoch 1:  62%|██████▏   | 18/29 [00:05<00:03,  3.31it/s, v_num=3.41e+7, train_loss_step=0.580, val_loss=0.946, train_loss_epoch=0.576]\n",
      "Epoch 1:  62%|██████▏   | 18/29 [00:05<00:03,  3.31it/s, v_num=3.41e+7, train_loss_step=0.366, val_loss=0.946, train_loss_epoch=0.576]\n",
      "Epoch 2:  21%|██        | 6/29 [00:02<00:09,  2.31it/s, v_num=3.41e+7, train_loss_step=0.337, val_loss=0.909, train_loss_epoch=0.518]\u001b[32m [repeated 23x across cluster]\u001b[0m\n",
      "Epoch 1:  93%|█████████▎| 27/29 [00:08<00:00,  3.07it/s, v_num=3.41e+7, train_loss_step=0.317, val_loss=0.946, train_loss_epoch=0.576]\n",
      "Epoch 1:  97%|█████████▋| 28/29 [00:09<00:00,  3.06it/s, v_num=3.41e+7, train_loss_step=0.459, val_loss=0.946, train_loss_epoch=0.576]\n",
      "Epoch 1: 100%|██████████| 29/29 [00:09<00:00,  3.13it/s, v_num=3.41e+7, train_loss_step=0.468, val_loss=0.946, train_loss_epoch=0.576]\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:01,  4.54it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:01,  3.95it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00,  3.83it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:01<00:00,  3.80it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:01<00:00,  3.72it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  3.92it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 29/29 [00:10<00:00,  2.67it/s, v_num=3.41e+7, train_loss_step=0.468, val_loss=0.891, train_loss_epoch=0.576]\n",
      "Epoch 2:  59%|█████▊    | 17/29 [00:07<00:05,  2.28it/s, v_num=3.41e+7, train_loss_step=0.303, val_loss=0.909, train_loss_epoch=0.518]\u001b[32m [repeated 19x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/73171dbb/checkpoint_000001)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 29/29 [00:11<00:00,  2.62it/s, v_num=3.41e+7, train_loss_step=0.468, val_loss=0.891, train_loss_epoch=0.485]\n",
      "Epoch 2:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.468, val_loss=0.891, train_loss_epoch=0.485]         \n",
      "Epoch 2:  93%|█████████▎| 27/29 [00:11<00:00,  2.29it/s, v_num=3.41e+7, train_loss_step=0.472, val_loss=0.909, train_loss_epoch=0.518]\n",
      "Epoch 2:  97%|█████████▋| 28/29 [00:12<00:00,  2.28it/s, v_num=3.41e+7, train_loss_step=0.543, val_loss=0.909, train_loss_epoch=0.518]\n",
      "Epoch 2: 100%|██████████| 29/29 [00:12<00:00,  2.34it/s, v_num=3.41e+7, train_loss_step=0.340, val_loss=0.909, train_loss_epoch=0.518]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:01,  3.51it/s]\u001b[A\n",
      "Epoch 2:  41%|████▏     | 12/29 [00:04<00:06,  2.65it/s, v_num=3.41e+7, train_loss_step=0.462, val_loss=0.891, train_loss_epoch=0.485]\u001b[32m [repeated 21x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:01,  3.12it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00,  3.08it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:01<00:00,  3.04it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:01<00:00,  2.97it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  3.14it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 29/29 [00:14<00:00,  2.02it/s, v_num=3.41e+7, train_loss_step=0.340, val_loss=0.706, train_loss_epoch=0.518]\n",
      "Epoch 2: 100%|██████████| 29/29 [00:14<00:00,  1.99it/s, v_num=3.41e+7, train_loss_step=0.340, val_loss=0.706, train_loss_epoch=0.433]\n",
      "Epoch 3:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.340, val_loss=0.706, train_loss_epoch=0.433]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/60537ef6/checkpoint_000002)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  24%|██▍       | 7/29 [00:03<00:09,  2.22it/s, v_num=3.41e+7, train_loss_step=0.450, val_loss=0.706, train_loss_epoch=0.433]\u001b[32m [repeated 21x across cluster]\u001b[0m\n",
      "Epoch 2:  93%|█████████▎| 27/29 [00:09<00:00,  2.70it/s, v_num=3.41e+7, train_loss_step=0.456, val_loss=0.891, train_loss_epoch=0.485]\n",
      "Epoch 2:  97%|█████████▋| 28/29 [00:10<00:00,  2.70it/s, v_num=3.41e+7, train_loss_step=0.524, val_loss=0.891, train_loss_epoch=0.485]\n",
      "Epoch 2: 100%|██████████| 29/29 [00:10<00:00,  2.77it/s, v_num=3.41e+7, train_loss_step=0.338, val_loss=0.891, train_loss_epoch=0.485]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:01,  4.63it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:01,  3.94it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00,  3.85it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:01<00:00,  3.79it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:01<00:00,  3.72it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  3.91it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 29/29 [00:12<00:00,  2.40it/s, v_num=3.41e+7, train_loss_step=0.338, val_loss=1.100, train_loss_epoch=0.485]\n",
      "Epoch 2: 100%|██████████| 29/29 [00:12<00:00,  2.36it/s, v_num=3.41e+7, train_loss_step=0.338, val_loss=1.100, train_loss_epoch=0.419]\n",
      "Epoch 3:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.338, val_loss=1.100, train_loss_epoch=0.419]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/73171dbb/checkpoint_000002)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  24%|██▍       | 7/29 [00:02<00:08,  2.69it/s, v_num=3.41e+7, train_loss_step=0.442, val_loss=1.100, train_loss_epoch=0.419]\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "Epoch 3:  93%|█████████▎| 27/29 [00:11<00:00,  2.29it/s, v_num=3.41e+7, train_loss_step=0.416, val_loss=0.706, train_loss_epoch=0.433]\n",
      "Epoch 3:  97%|█████████▋| 28/29 [00:12<00:00,  2.28it/s, v_num=3.41e+7, train_loss_step=0.268, val_loss=0.706, train_loss_epoch=0.433]\n",
      "Epoch 3: 100%|██████████| 29/29 [00:12<00:00,  2.35it/s, v_num=3.41e+7, train_loss_step=0.411, val_loss=0.706, train_loss_epoch=0.433]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:01,  3.40it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:01,  3.12it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00,  3.02it/s]\u001b[A\n",
      "Epoch 3:  69%|██████▉   | 20/29 [00:07<00:03,  2.77it/s, v_num=3.41e+7, train_loss_step=0.423, val_loss=1.100, train_loss_epoch=0.419]\u001b[32m [repeated 21x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:01<00:00,  3.00it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:01<00:00,  2.95it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  3.11it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 29/29 [00:14<00:00,  2.02it/s, v_num=3.41e+7, train_loss_step=0.411, val_loss=0.833, train_loss_epoch=0.433]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/60537ef6/checkpoint_000003)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 29/29 [00:14<00:00,  1.99it/s, v_num=3.41e+7, train_loss_step=0.411, val_loss=0.833, train_loss_epoch=0.423]\n",
      "Epoch 4:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.411, val_loss=0.833, train_loss_epoch=0.423]         \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 22:38:09,010\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 29/29 [00:09<00:00,  2.94it/s, v_num=3.41e+7, train_loss_step=0.417, val_loss=1.100, train_loss_epoch=0.419]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00,  5.43it/s]\u001b[A\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Epoch 4:  17%|█▋        | 5/29 [00:01<00:06,  3.61it/s, v_num=3.41e+7, train_loss_step=0.379, val_loss=0.427, train_loss_epoch=0.417]\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  5.72it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 29/29 [00:10<00:00,  2.64it/s, v_num=3.41e+7, train_loss_step=0.417, val_loss=0.427, train_loss_epoch=0.419]\n",
      "Epoch 3: 100%|██████████| 29/29 [00:11<00:00,  2.60it/s, v_num=3.41e+7, train_loss_step=0.417, val_loss=0.427, train_loss_epoch=0.417]\n",
      "Epoch 4:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.417, val_loss=0.427, train_loss_epoch=0.417]         \n",
      "Epoch 4:  69%|██████▉   | 20/29 [00:06<00:02,  3.11it/s, v_num=3.41e+7, train_loss_step=0.216, val_loss=0.427, train_loss_epoch=0.417]\u001b[32m [repeated 28x across cluster]\u001b[0m\n",
      "Epoch 4:  93%|█████████▎| 27/29 [00:09<00:00,  2.79it/s, v_num=3.41e+7, train_loss_step=0.460, val_loss=0.833, train_loss_epoch=0.423]\n",
      "Epoch 4:  97%|█████████▋| 28/29 [00:10<00:00,  2.80it/s, v_num=3.41e+7, train_loss_step=0.372, val_loss=0.833, train_loss_epoch=0.423]\n",
      "Epoch 4: 100%|██████████| 29/29 [00:10<00:00,  2.87it/s, v_num=3.41e+7, train_loss_step=0.241, val_loss=0.833, train_loss_epoch=0.423]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:01,  3.49it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:01,  3.19it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00,  3.12it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:01<00:00,  3.07it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  3.20it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 29/29 [00:12<00:00,  2.41it/s, v_num=3.41e+7, train_loss_step=0.241, val_loss=0.360, train_loss_epoch=0.423]\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Epoch 4: 100%|██████████| 29/29 [00:12<00:00,  2.37it/s, v_num=3.41e+7, train_loss_step=0.241, val_loss=0.360, train_loss_epoch=0.378]\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Epoch 5:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.241, val_loss=0.360, train_loss_epoch=0.378]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/60537ef6/checkpoint_000004)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 22:38:19,661\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  17%|█▋        | 5/29 [00:01<00:07,  3.12it/s, v_num=3.41e+7, train_loss_step=0.777, val_loss=0.360, train_loss_epoch=0.378]\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "Epoch 4: 100%|██████████| 29/29 [00:09<00:00,  3.22it/s, v_num=3.41e+7, train_loss_step=0.231, val_loss=0.427, train_loss_epoch=0.417]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:01<00:00,  4.40it/s]\u001b[A\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  4.72it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 29/29 [00:10<00:00,  2.80it/s, v_num=3.41e+7, train_loss_step=0.231, val_loss=0.594, train_loss_epoch=0.417]\n",
      "Epoch 4: 100%|██████████| 29/29 [00:10<00:00,  2.75it/s, v_num=3.41e+7, train_loss_step=0.231, val_loss=0.594, train_loss_epoch=0.364]\n",
      "Epoch 5:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.231, val_loss=0.594, train_loss_epoch=0.364]         \n",
      "Epoch 5:  62%|██████▏   | 18/29 [00:05<00:03,  3.09it/s, v_num=3.41e+7, train_loss_step=0.305, val_loss=0.594, train_loss_epoch=0.364]\u001b[32m [repeated 28x across cluster]\u001b[0m\n",
      "Epoch 5:  93%|█████████▎| 27/29 [00:09<00:00,  2.95it/s, v_num=3.41e+7, train_loss_step=0.294, val_loss=0.594, train_loss_epoch=0.364]\n",
      "Epoch 5:  97%|█████████▋| 28/29 [00:09<00:00,  2.95it/s, v_num=3.41e+7, train_loss_step=0.338, val_loss=0.594, train_loss_epoch=0.364]\n",
      "Epoch 5: 100%|██████████| 29/29 [00:09<00:00,  3.02it/s, v_num=3.41e+7, train_loss_step=0.291, val_loss=0.594, train_loss_epoch=0.364]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:01,  4.54it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:01,  3.92it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00,  3.83it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Epoch 5:  90%|████████▉ | 26/29 [00:10<00:01,  2.55it/s, v_num=3.41e+7, train_loss_step=0.320, val_loss=0.360, train_loss_epoch=0.378]\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  3.94it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 29/29 [00:11<00:00,  2.59it/s, v_num=3.41e+7, train_loss_step=0.291, val_loss=0.276, train_loss_epoch=0.364]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/73171dbb/checkpoint_000005)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2025-03-11 22:38:31,247\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Epoch 5: 100%|██████████| 29/29 [00:11<00:00,  2.54it/s, v_num=3.41e+7, train_loss_step=0.291, val_loss=0.276, train_loss_epoch=0.371]\n",
      "Epoch 6:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.291, val_loss=0.276, train_loss_epoch=0.371]         \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 22:38:32,204\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Epoch 5: 100%|██████████| 29/29 [00:11<00:00,  2.61it/s, v_num=3.41e+7, train_loss_step=0.441, val_loss=0.360, train_loss_epoch=0.378]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:01<00:00,  3.14it/s]\u001b[A\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "Epoch 6:  38%|███▊      | 11/29 [00:03<00:05,  3.00it/s, v_num=3.41e+7, train_loss_step=0.462, val_loss=0.409, train_loss_epoch=0.372]\u001b[32m [repeated 26x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  3.30it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 29/29 [00:13<00:00,  2.23it/s, v_num=3.41e+7, train_loss_step=0.441, val_loss=0.409, train_loss_epoch=0.378]\n",
      "Epoch 5: 100%|██████████| 29/29 [00:13<00:00,  2.20it/s, v_num=3.41e+7, train_loss_step=0.441, val_loss=0.409, train_loss_epoch=0.372]\n",
      "Epoch 6:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.441, val_loss=0.409, train_loss_epoch=0.372]         \n",
      "Epoch 6:  93%|█████████▎| 27/29 [00:08<00:00,  3.21it/s, v_num=3.41e+7, train_loss_step=0.304, val_loss=0.276, train_loss_epoch=0.371]\n",
      "Epoch 6:  97%|█████████▋| 28/29 [00:08<00:00,  3.23it/s, v_num=3.41e+7, train_loss_step=0.263, val_loss=0.276, train_loss_epoch=0.371]\n",
      "Epoch 6: 100%|██████████| 29/29 [00:08<00:00,  3.31it/s, v_num=3.41e+7, train_loss_step=0.648, val_loss=0.276, train_loss_epoch=0.371]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00,  6.50it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00,  5.69it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00,  5.51it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00<00:00,  5.45it/s]\u001b[A\n",
      "Epoch 6:  86%|████████▌ | 25/29 [00:08<00:01,  2.85it/s, v_num=3.41e+7, train_loss_step=0.182, val_loss=0.409, train_loss_epoch=0.372]\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00,  5.37it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  5.65it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 29/29 [00:09<00:00,  2.94it/s, v_num=3.41e+7, train_loss_step=0.648, val_loss=0.604, train_loss_epoch=0.371]\n",
      "Epoch 6: 100%|██████████| 29/29 [00:10<00:00,  2.87it/s, v_num=3.41e+7, train_loss_step=0.648, val_loss=0.604, train_loss_epoch=0.349]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/73171dbb/checkpoint_000006)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.648, val_loss=0.604, train_loss_epoch=0.349]         \n",
      "Epoch 6: 100%|██████████| 29/29 [00:09<00:00,  2.93it/s, v_num=3.41e+7, train_loss_step=0.641, val_loss=0.409, train_loss_epoch=0.372]\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 22:38:43,756\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Epoch 6:  93%|█████████▎| 27/29 [00:09<00:00,  2.85it/s, v_num=3.41e+7, train_loss_step=0.305, val_loss=0.409, train_loss_epoch=0.372]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:01<00:00,  4.36it/s]\u001b[A\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Epoch 7:  59%|█████▊    | 17/29 [00:04<00:03,  3.69it/s, v_num=3.41e+7, train_loss_step=0.440, val_loss=0.604, train_loss_epoch=0.349]\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  4.60it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 29/29 [00:11<00:00,  2.58it/s, v_num=3.41e+7, train_loss_step=0.641, val_loss=0.748, train_loss_epoch=0.372]\n",
      "Epoch 6: 100%|██████████| 29/29 [00:11<00:00,  2.54it/s, v_num=3.41e+7, train_loss_step=0.641, val_loss=0.748, train_loss_epoch=0.363]\n",
      "Epoch 7:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.641, val_loss=0.748, train_loss_epoch=0.363]         \n",
      "Epoch 7:  93%|█████████▎| 27/29 [00:07<00:00,  3.68it/s, v_num=3.41e+7, train_loss_step=0.478, val_loss=0.604, train_loss_epoch=0.349]\n",
      "Epoch 7:  97%|█████████▋| 28/29 [00:07<00:00,  3.63it/s, v_num=3.41e+7, train_loss_step=0.368, val_loss=0.604, train_loss_epoch=0.349]\n",
      "Epoch 7: 100%|██████████| 29/29 [00:07<00:00,  3.71it/s, v_num=3.41e+7, train_loss_step=0.782, val_loss=0.604, train_loss_epoch=0.349]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:01<00:00,  3.80it/s]\u001b[A\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  3.91it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 29/29 [00:09<00:00,  3.08it/s, v_num=3.41e+7, train_loss_step=0.782, val_loss=0.318, train_loss_epoch=0.349]\n",
      "Epoch 7: 100%|██████████| 29/29 [00:09<00:00,  3.01it/s, v_num=3.41e+7, train_loss_step=0.782, val_loss=0.318, train_loss_epoch=0.362]\n",
      "Epoch 7:  72%|███████▏  | 21/29 [00:07<00:02,  2.87it/s, v_num=3.41e+7, train_loss_step=0.191, val_loss=0.748, train_loss_epoch=0.363]\u001b[32m [repeated 23x across cluster]\u001b[0m\n",
      "Epoch 8:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.782, val_loss=0.318, train_loss_epoch=0.362]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/73171dbb/checkpoint_000007)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:  93%|█████████▎| 27/29 [00:09<00:00,  2.73it/s, v_num=3.41e+7, train_loss_step=0.436, val_loss=0.748, train_loss_epoch=0.363]\n",
      "Epoch 7:  97%|█████████▋| 28/29 [00:10<00:00,  2.71it/s, v_num=3.41e+7, train_loss_step=0.343, val_loss=0.748, train_loss_epoch=0.363]\n",
      "Epoch 7: 100%|██████████| 29/29 [00:10<00:00,  2.77it/s, v_num=3.41e+7, train_loss_step=0.947, val_loss=0.748, train_loss_epoch=0.363]\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:01<00:00,  3.13it/s]\u001b[A\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  3.23it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 29/29 [00:12<00:00,  2.34it/s, v_num=3.41e+7, train_loss_step=0.947, val_loss=0.377, train_loss_epoch=0.363]\n",
      "Epoch 8:  45%|████▍     | 13/29 [00:04<00:05,  2.74it/s, v_num=3.41e+7, train_loss_step=0.345, val_loss=0.318, train_loss_epoch=0.362]\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "Epoch 7: 100%|██████████| 29/29 [00:12<00:00,  2.31it/s, v_num=3.41e+7, train_loss_step=0.947, val_loss=0.377, train_loss_epoch=0.365]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/60537ef6/checkpoint_000007)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.947, val_loss=0.377, train_loss_epoch=0.365]         \n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:01<00:00,  3.06it/s]\u001b[A\n",
      "Epoch 8:  93%|█████████▎| 27/29 [00:09<00:00,  2.75it/s, v_num=3.41e+7, train_loss_step=0.329, val_loss=0.318, train_loss_epoch=0.362]\n",
      "Epoch 8:  41%|████▏     | 12/29 [00:05<00:07,  2.38it/s, v_num=3.41e+7, train_loss_step=0.328, val_loss=0.377, train_loss_epoch=0.365]\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "Epoch 8:  97%|█████████▋| 28/29 [00:10<00:00,  2.74it/s, v_num=3.41e+7, train_loss_step=0.344, val_loss=0.318, train_loss_epoch=0.362]\n",
      "Epoch 8: 100%|██████████| 29/29 [00:10<00:00,  2.81it/s, v_num=3.41e+7, train_loss_step=0.267, val_loss=0.318, train_loss_epoch=0.362]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:01,  4.38it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:01,  3.89it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00,  3.76it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:01<00:00,  3.73it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:01<00:00,  3.66it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  3.87it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 29/29 [00:11<00:00,  2.43it/s, v_num=3.41e+7, train_loss_step=0.267, val_loss=0.324, train_loss_epoch=0.362]\n",
      "Epoch 8: 100%|██████████| 29/29 [00:12<00:00,  2.39it/s, v_num=3.41e+7, train_loss_step=0.267, val_loss=0.324, train_loss_epoch=0.337]\n",
      "Epoch 9:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.267, val_loss=0.324, train_loss_epoch=0.337]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/73171dbb/checkpoint_000008)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  14%|█▍        | 4/29 [00:01<00:09,  2.68it/s, v_num=3.41e+7, train_loss_step=0.211, val_loss=0.324, train_loss_epoch=0.337]\n",
      "Epoch 9:  14%|█▍        | 4/29 [00:01<00:09,  2.68it/s, v_num=3.41e+7, train_loss_step=0.350, val_loss=0.324, train_loss_epoch=0.337]\n",
      "Epoch 9:  28%|██▊       | 8/29 [00:02<00:07,  2.69it/s, v_num=3.41e+7, train_loss_step=0.218, val_loss=0.324, train_loss_epoch=0.337]\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "Epoch 8:  93%|█████████▎| 27/29 [00:11<00:00,  2.38it/s, v_num=3.41e+7, train_loss_step=0.340, val_loss=0.377, train_loss_epoch=0.365]\n",
      "Epoch 8: 100%|██████████| 29/29 [00:11<00:00,  2.47it/s, v_num=3.41e+7, train_loss_step=0.361, val_loss=0.377, train_loss_epoch=0.365]\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00,  5.27it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00,  4.68it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00,  4.63it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00<00:00,  4.60it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:01<00:00,  4.52it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  4.76it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 29/29 [00:13<00:00,  2.23it/s, v_num=3.41e+7, train_loss_step=0.361, val_loss=0.296, train_loss_epoch=0.365]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/60537ef6/checkpoint_000008)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 29/29 [00:13<00:00,  2.19it/s, v_num=3.41e+7, train_loss_step=0.361, val_loss=0.296, train_loss_epoch=0.351]\n",
      "Epoch 9:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.361, val_loss=0.296, train_loss_epoch=0.351]         \n",
      "Epoch 9:  83%|████████▎ | 24/29 [00:07<00:01,  3.03it/s, v_num=3.41e+7, train_loss_step=0.233, val_loss=0.324, train_loss_epoch=0.337]\u001b[32m [repeated 23x across cluster]\u001b[0m\n",
      "Epoch 9:  97%|█████████▋| 28/29 [00:09<00:00,  2.97it/s, v_num=3.41e+7, train_loss_step=0.317, val_loss=0.324, train_loss_epoch=0.337]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:01,  4.46it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:01,  3.93it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00,  3.81it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:01<00:00,  3.76it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:01<00:00,  3.69it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  3.88it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 29/29 [00:11<00:00,  2.60it/s, v_num=3.41e+7, train_loss_step=0.164, val_loss=0.978, train_loss_epoch=0.337]\n",
      "Epoch 9: 100%|██████████| 29/29 [00:11<00:00,  2.55it/s, v_num=3.41e+7, train_loss_step=0.164, val_loss=0.978, train_loss_epoch=0.309]\n",
      "Epoch 10:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.164, val_loss=0.978, train_loss_epoch=0.309]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/73171dbb/checkpoint_000009)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  21%|██        | 6/29 [00:01<00:06,  3.74it/s, v_num=3.41e+7, train_loss_step=0.263, val_loss=0.978, train_loss_epoch=0.309]\u001b[32m [repeated 21x across cluster]\u001b[0m\n",
      "Epoch 9: 100%|██████████| 29/29 [00:09<00:00,  3.03it/s, v_num=3.41e+7, train_loss_step=0.164, val_loss=0.324, train_loss_epoch=0.337]\n",
      "Epoch 9:  93%|█████████▎| 27/29 [00:10<00:00,  2.49it/s, v_num=3.41e+7, train_loss_step=0.244, val_loss=0.296, train_loss_epoch=0.351]\n",
      "Epoch 9:  97%|█████████▋| 28/29 [00:11<00:00,  2.48it/s, v_num=3.41e+7, train_loss_step=0.321, val_loss=0.296, train_loss_epoch=0.351]\n",
      "Epoch 9: 100%|██████████| 29/29 [00:11<00:00,  2.54it/s, v_num=3.41e+7, train_loss_step=0.227, val_loss=0.296, train_loss_epoch=0.351]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00,  5.26it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00,  4.74it/s]\u001b[A\n",
      "Epoch 10:  66%|██████▌   | 19/29 [00:06<00:03,  2.99it/s, v_num=3.41e+7, train_loss_step=0.285, val_loss=0.978, train_loss_epoch=0.309]\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00,  4.65it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00<00:00,  4.58it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:01<00:00,  4.49it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  4.71it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 29/29 [00:12<00:00,  2.28it/s, v_num=3.41e+7, train_loss_step=0.227, val_loss=1.250, train_loss_epoch=0.351]\n",
      "Epoch 9: 100%|██████████| 29/29 [00:12<00:00,  2.24it/s, v_num=3.41e+7, train_loss_step=0.227, val_loss=1.250, train_loss_epoch=0.326]\n",
      "Epoch 10:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.227, val_loss=1.250, train_loss_epoch=0.326]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/60537ef6/checkpoint_000009)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 29/29 [00:09<00:00,  3.19it/s, v_num=3.41e+7, train_loss_step=0.225, val_loss=0.978, train_loss_epoch=0.309]\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 22:39:25,664\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/73171dbb/checkpoint_000010)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  93%|█████████▎| 27/29 [00:08<00:00,  3.11it/s, v_num=3.41e+7, train_loss_step=0.249, val_loss=0.978, train_loss_epoch=0.309]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00,  5.21it/s]\u001b[A\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Epoch 11:  14%|█▍        | 4/29 [00:01<00:07,  3.39it/s, v_num=3.41e+7, train_loss_step=0.657, val_loss=0.250, train_loss_epoch=0.296]\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  5.49it/s]\u001b[A\n",
      "Epoch 10: 100%|██████████| 29/29 [00:10<00:00,  2.83it/s, v_num=3.41e+7, train_loss_step=0.225, val_loss=0.250, train_loss_epoch=0.309]\n",
      "Epoch 10: 100%|██████████| 29/29 [00:10<00:00,  2.78it/s, v_num=3.41e+7, train_loss_step=0.225, val_loss=0.250, train_loss_epoch=0.296]\n",
      "Epoch 11:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.225, val_loss=0.250, train_loss_epoch=0.296]         \n",
      "Epoch 10:  93%|█████████▎| 27/29 [00:08<00:00,  3.00it/s, v_num=3.41e+7, train_loss_step=0.253, val_loss=1.250, train_loss_epoch=0.326]\n",
      "Epoch 11:  72%|███████▏  | 21/29 [00:06<00:02,  3.49it/s, v_num=3.41e+7, train_loss_step=0.220, val_loss=0.250, train_loss_epoch=0.296]\u001b[32m [repeated 32x across cluster]\u001b[0m\n",
      "Epoch 10:  97%|█████████▋| 28/29 [00:09<00:00,  2.97it/s, v_num=3.41e+7, train_loss_step=0.327, val_loss=1.250, train_loss_epoch=0.326]\n",
      "Epoch 10: 100%|██████████| 29/29 [00:09<00:00,  3.03it/s, v_num=3.41e+7, train_loss_step=0.218, val_loss=1.250, train_loss_epoch=0.326]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:01,  3.64it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:01,  3.18it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00,  3.13it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:01<00:00,  3.11it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:01<00:00,  3.03it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  3.17it/s]\u001b[A\n",
      "Epoch 10: 100%|██████████| 29/29 [00:11<00:00,  2.52it/s, v_num=3.41e+7, train_loss_step=0.218, val_loss=0.292, train_loss_epoch=0.326]\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Epoch 10: 100%|██████████| 29/29 [00:11<00:00,  2.48it/s, v_num=3.41e+7, train_loss_step=0.218, val_loss=0.292, train_loss_epoch=0.315]\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Epoch 11:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.218, val_loss=0.292, train_loss_epoch=0.315]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/60537ef6/checkpoint_000010)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 22:39:35,828\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/73171dbb/checkpoint_000011)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 29/29 [00:08<00:00,  3.33it/s, v_num=3.41e+7, train_loss_step=0.121, val_loss=0.250, train_loss_epoch=0.296]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Epoch 12:  14%|█▍        | 4/29 [00:01<00:06,  3.74it/s, v_num=3.41e+7, train_loss_step=0.222, val_loss=0.270, train_loss_epoch=0.282]\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00,  5.19it/s]\u001b[A\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  5.48it/s]\u001b[A\n",
      "Epoch 11: 100%|██████████| 29/29 [00:09<00:00,  2.95it/s, v_num=3.41e+7, train_loss_step=0.121, val_loss=0.270, train_loss_epoch=0.296]\n",
      "Epoch 11: 100%|██████████| 29/29 [00:10<00:00,  2.88it/s, v_num=3.41e+7, train_loss_step=0.121, val_loss=0.270, train_loss_epoch=0.282]\n",
      "Epoch 12:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.121, val_loss=0.270, train_loss_epoch=0.282]         \n",
      "Epoch 11:  69%|██████▉   | 20/29 [00:07<00:03,  2.71it/s, v_num=3.41e+7, train_loss_step=0.303, val_loss=0.292, train_loss_epoch=0.315]\u001b[32m [repeated 28x across cluster]\u001b[0m\n",
      "Epoch 12:  93%|█████████▎| 27/29 [00:09<00:00,  2.97it/s, v_num=3.41e+7, train_loss_step=0.320, val_loss=0.270, train_loss_epoch=0.282]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:01,  4.59it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 22:39:47,359\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  3.94it/s]\u001b[A\n",
      "Epoch 12: 100%|██████████| 29/29 [00:11<00:00,  2.60it/s, v_num=3.41e+7, train_loss_step=0.309, val_loss=0.273, train_loss_epoch=0.282]\n",
      "Epoch 11:  90%|████████▉ | 26/29 [00:09<00:01,  2.61it/s, v_num=3.41e+7, train_loss_step=0.287, val_loss=0.292, train_loss_epoch=0.315]\u001b[32m [repeated 14x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/73171dbb/checkpoint_000012)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Epoch 12: 100%|██████████| 29/29 [00:11<00:00,  2.55it/s, v_num=3.41e+7, train_loss_step=0.309, val_loss=0.273, train_loss_epoch=0.275]\n",
      "Epoch 13:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.309, val_loss=0.273, train_loss_epoch=0.275]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 22:39:47,752\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/60537ef6/checkpoint_000011)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 29/29 [00:10<00:00,  2.65it/s, v_num=3.41e+7, train_loss_step=0.138, val_loss=0.292, train_loss_epoch=0.315]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:01<00:00,  3.11it/s]\u001b[A\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "Epoch 12:  38%|███▊      | 11/29 [00:04<00:07,  2.42it/s, v_num=3.41e+7, train_loss_step=0.579, val_loss=0.313, train_loss_epoch=0.301]\n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  3.35it/s]\u001b[A\n",
      "Epoch 11: 100%|██████████| 29/29 [00:12<00:00,  2.26it/s, v_num=3.41e+7, train_loss_step=0.138, val_loss=0.313, train_loss_epoch=0.315]\n",
      "Epoch 13:  45%|████▍     | 13/29 [00:04<00:05,  2.85it/s, v_num=3.41e+7, train_loss_step=0.314, val_loss=0.273, train_loss_epoch=0.275]\u001b[32m [repeated 23x across cluster]\u001b[0m\n",
      "Epoch 11: 100%|██████████| 29/29 [00:12<00:00,  2.23it/s, v_num=3.41e+7, train_loss_step=0.138, val_loss=0.313, train_loss_epoch=0.301]\n",
      "Epoch 12:  38%|███▊      | 11/29 [00:04<00:07,  2.42it/s, v_num=3.41e+7, train_loss_step=0.232, val_loss=0.313, train_loss_epoch=0.301]\n",
      "Epoch 12:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.138, val_loss=0.313, train_loss_epoch=0.301]         \n",
      "Epoch 13:  93%|█████████▎| 27/29 [00:09<00:00,  2.92it/s, v_num=3.41e+7, train_loss_step=0.208, val_loss=0.273, train_loss_epoch=0.275]\n",
      "Epoch 13:  97%|█████████▋| 28/29 [00:09<00:00,  2.94it/s, v_num=3.41e+7, train_loss_step=0.353, val_loss=0.273, train_loss_epoch=0.275]\n",
      "Epoch 13: 100%|██████████| 29/29 [00:09<00:00,  3.02it/s, v_num=3.41e+7, train_loss_step=0.176, val_loss=0.273, train_loss_epoch=0.275]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00,  6.65it/s]\u001b[A\n",
      "Epoch 12:  86%|████████▌ | 25/29 [00:09<00:01,  2.59it/s, v_num=3.41e+7, train_loss_step=0.217, val_loss=0.313, train_loss_epoch=0.301]\u001b[32m [repeated 27x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00,  5.71it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00,  5.58it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00<00:00,  5.47it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00,  5.36it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  5.66it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Epoch 13: 100%|██████████| 29/29 [00:10<00:00,  2.71it/s, v_num=3.41e+7, train_loss_step=0.176, val_loss=0.426, train_loss_epoch=0.275]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/73171dbb/checkpoint_000013)\n",
      "2025-03-11 22:39:58,395\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 29/29 [00:10<00:00,  2.66it/s, v_num=3.41e+7, train_loss_step=0.176, val_loss=0.426, train_loss_epoch=0.275]\n",
      "Epoch 14:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.176, val_loss=0.426, train_loss_epoch=0.275]         \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 22:40:00,636\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/60537ef6/checkpoint_000012)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 29/29 [00:10<00:00,  2.71it/s, v_num=3.41e+7, train_loss_step=0.286, val_loss=0.313, train_loss_epoch=0.301]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:01<00:00,  3.19it/s]\u001b[A\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Epoch 14:  45%|████▍     | 13/29 [00:04<00:05,  3.20it/s, v_num=3.41e+7, train_loss_step=0.280, val_loss=0.426, train_loss_epoch=0.275]\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  3.36it/s]\u001b[A\n",
      "Epoch 12: 100%|██████████| 29/29 [00:12<00:00,  2.31it/s, v_num=3.41e+7, train_loss_step=0.286, val_loss=0.257, train_loss_epoch=0.301]\n",
      "Epoch 12: 100%|██████████| 29/29 [00:12<00:00,  2.28it/s, v_num=3.41e+7, train_loss_step=0.286, val_loss=0.257, train_loss_epoch=0.301]\n",
      "Epoch 13:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.286, val_loss=0.257, train_loss_epoch=0.301]         \n",
      "Epoch 14:  93%|█████████▎| 27/29 [00:08<00:00,  3.10it/s, v_num=3.41e+7, train_loss_step=0.225, val_loss=0.426, train_loss_epoch=0.275]\n",
      "Epoch 14:  97%|█████████▋| 28/29 [00:09<00:00,  3.08it/s, v_num=3.41e+7, train_loss_step=0.260, val_loss=0.426, train_loss_epoch=0.275]\n",
      "Epoch 13:  62%|██████▏   | 18/29 [00:06<00:04,  2.74it/s, v_num=3.41e+7, train_loss_step=0.162, val_loss=0.257, train_loss_epoch=0.301]\u001b[32m [repeated 26x across cluster]\u001b[0m\n",
      "Epoch 14: 100%|██████████| 29/29 [00:09<00:00,  3.16it/s, v_num=3.41e+7, train_loss_step=0.145, val_loss=0.426, train_loss_epoch=0.275]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:01,  4.61it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00,  4.70it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00,  4.33it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00<00:00,  4.14it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:01<00:00,  3.99it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  4.16it/s]\u001b[A\n",
      "Epoch 14: 100%|██████████| 29/29 [00:10<00:00,  2.71it/s, v_num=3.41e+7, train_loss_step=0.145, val_loss=0.205, train_loss_epoch=0.275]\n",
      "Epoch 14: 100%|██████████| 29/29 [00:10<00:00,  2.66it/s, v_num=3.41e+7, train_loss_step=0.145, val_loss=0.205, train_loss_epoch=0.263]\n",
      "Epoch 15:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.145, val_loss=0.205, train_loss_epoch=0.263]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/73171dbb/checkpoint_000014)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Epoch 13: 100%|██████████| 29/29 [00:10<00:00,  2.65it/s, v_num=3.41e+7, train_loss_step=0.178, val_loss=0.257, train_loss_epoch=0.301]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Epoch 15:  28%|██▊       | 8/29 [00:02<00:07,  2.68it/s, v_num=3.41e+7, train_loss_step=0.328, val_loss=0.205, train_loss_epoch=0.263]\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:01<00:00,  3.14it/s]\u001b[A\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 22:40:13,788\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/60537ef6/checkpoint_000013)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  3.24it/s]\u001b[A\n",
      "Epoch 13: 100%|██████████| 29/29 [00:12<00:00,  2.26it/s, v_num=3.41e+7, train_loss_step=0.178, val_loss=0.510, train_loss_epoch=0.301]\n",
      "Epoch 13: 100%|██████████| 29/29 [00:13<00:00,  2.22it/s, v_num=3.41e+7, train_loss_step=0.178, val_loss=0.510, train_loss_epoch=0.298]\n",
      "Epoch 14:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.178, val_loss=0.510, train_loss_epoch=0.298]         \n",
      "Epoch 14:  41%|████▏     | 12/29 [00:03<00:05,  3.23it/s, v_num=3.41e+7, train_loss_step=0.293, val_loss=0.510, train_loss_epoch=0.298]\u001b[32m [repeated 29x across cluster]\u001b[0m\n",
      "Epoch 15:  93%|█████████▎| 27/29 [00:08<00:00,  3.24it/s, v_num=3.41e+7, train_loss_step=0.223, val_loss=0.205, train_loss_epoch=0.263]\n",
      "Epoch 15:  97%|█████████▋| 28/29 [00:08<00:00,  3.22it/s, v_num=3.41e+7, train_loss_step=0.219, val_loss=0.205, train_loss_epoch=0.263]\n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:01<00:00,  3.07it/s]\u001b[A\n",
      "Epoch 15: 100%|██████████| 29/29 [00:08<00:00,  3.29it/s, v_num=3.41e+7, train_loss_step=0.224, val_loss=0.205, train_loss_epoch=0.263]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:01,  4.45it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:01,  3.91it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00,  3.81it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:01<00:00,  3.77it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:01<00:00,  3.70it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  3.89it/s]\u001b[A\n",
      "Epoch 15: 100%|██████████| 29/29 [00:10<00:00,  2.79it/s, v_num=3.41e+7, train_loss_step=0.224, val_loss=0.247, train_loss_epoch=0.263]\n",
      "Epoch 15: 100%|██████████| 29/29 [00:10<00:00,  2.73it/s, v_num=3.41e+7, train_loss_step=0.224, val_loss=0.247, train_loss_epoch=0.261]\n",
      "Epoch 16:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.224, val_loss=0.247, train_loss_epoch=0.261]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/73171dbb/checkpoint_000015)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:  24%|██▍       | 7/29 [00:02<00:08,  2.73it/s, v_num=3.41e+7, train_loss_step=0.147, val_loss=0.247, train_loss_epoch=0.261]\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "Epoch 14:  93%|█████████▎| 27/29 [00:09<00:00,  2.71it/s, v_num=3.41e+7, train_loss_step=0.242, val_loss=0.510, train_loss_epoch=0.298]\n",
      "Epoch 14:  97%|█████████▋| 28/29 [00:10<00:00,  2.70it/s, v_num=3.41e+7, train_loss_step=0.303, val_loss=0.510, train_loss_epoch=0.298]\n",
      "Epoch 14: 100%|██████████| 29/29 [00:10<00:00,  2.77it/s, v_num=3.41e+7, train_loss_step=0.0896, val_loss=0.510, train_loss_epoch=0.298]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:01,  3.60it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:01,  3.21it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00,  3.16it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:01<00:00,  3.14it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:01<00:00,  3.06it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  3.24it/s]\u001b[A\n",
      "Epoch 14: 100%|██████████| 29/29 [00:12<00:00,  2.34it/s, v_num=3.41e+7, train_loss_step=0.0896, val_loss=0.230, train_loss_epoch=0.298]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/60537ef6/checkpoint_000014)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 29/29 [00:12<00:00,  2.30it/s, v_num=3.41e+7, train_loss_step=0.0896, val_loss=0.230, train_loss_epoch=0.284]\n",
      "Epoch 15:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.0896, val_loss=0.230, train_loss_epoch=0.284]         \n",
      "Epoch 15:  17%|█▋        | 5/29 [00:01<00:07,  3.22it/s, v_num=3.41e+7, train_loss_step=0.262, val_loss=0.230, train_loss_epoch=0.284]\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "Epoch 16:  93%|█████████▎| 27/29 [00:08<00:00,  3.06it/s, v_num=3.41e+7, train_loss_step=0.196, val_loss=0.247, train_loss_epoch=0.261]\n",
      "Epoch 16:  97%|█████████▋| 28/29 [00:09<00:00,  3.08it/s, v_num=3.41e+7, train_loss_step=0.191, val_loss=0.247, train_loss_epoch=0.261]\n",
      "Epoch 16: 100%|██████████| 29/29 [00:09<00:00,  3.15it/s, v_num=3.41e+7, train_loss_step=0.573, val_loss=0.247, train_loss_epoch=0.261]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00,  5.89it/s]\u001b[A\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 22:40:30,820\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  5.78it/s]\u001b[A\n",
      "Epoch 16: 100%|██████████| 29/29 [00:10<00:00,  2.82it/s, v_num=3.41e+7, train_loss_step=0.573, val_loss=0.217, train_loss_epoch=0.261]\n",
      "Epoch 16: 100%|██████████| 29/29 [00:10<00:00,  2.77it/s, v_num=3.41e+7, train_loss_step=0.573, val_loss=0.217, train_loss_epoch=0.256]\n",
      "Epoch 17:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.573, val_loss=0.217, train_loss_epoch=0.256]         \n",
      "Epoch 15:  69%|██████▉   | 20/29 [00:06<00:03,  2.94it/s, v_num=3.41e+7, train_loss_step=0.293, val_loss=0.230, train_loss_epoch=0.284]\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00,  5.51it/s]\u001b[A\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Epoch 15:  93%|█████████▎| 27/29 [00:09<00:00,  2.74it/s, v_num=3.41e+7, train_loss_step=0.256, val_loss=0.230, train_loss_epoch=0.284]\n",
      "Epoch 15:  97%|█████████▋| 28/29 [00:10<00:00,  2.72it/s, v_num=3.41e+7, train_loss_step=0.223, val_loss=0.230, train_loss_epoch=0.284]\n",
      "Epoch 15: 100%|██████████| 29/29 [00:10<00:00,  2.79it/s, v_num=3.41e+7, train_loss_step=0.272, val_loss=0.230, train_loss_epoch=0.284]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Epoch 17:  69%|██████▉   | 20/29 [00:07<00:03,  2.75it/s, v_num=3.41e+7, train_loss_step=0.223, val_loss=0.217, train_loss_epoch=0.256]\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  3.23it/s]\u001b[A\n",
      "Epoch 15: 100%|██████████| 29/29 [00:12<00:00,  2.35it/s, v_num=3.41e+7, train_loss_step=0.272, val_loss=0.238, train_loss_epoch=0.284]\n",
      "Epoch 15: 100%|██████████| 29/29 [00:12<00:00,  2.31it/s, v_num=3.41e+7, train_loss_step=0.272, val_loss=0.238, train_loss_epoch=0.281]\n",
      "Epoch 16:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.272, val_loss=0.238, train_loss_epoch=0.281]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/60537ef6/checkpoint_000015)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:01<00:00,  3.07it/s]\u001b[A\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Epoch 17: 100%|██████████| 29/29 [00:09<00:00,  2.97it/s, v_num=3.41e+7, train_loss_step=0.0946, val_loss=0.217, train_loss_epoch=0.256]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 22:40:42,598\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:  10%|█         | 3/29 [00:00<00:06,  3.74it/s, v_num=3.41e+7, train_loss_step=0.231, val_loss=0.193, train_loss_epoch=0.238]\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  3.91it/s]\u001b[A\n",
      "Epoch 17: 100%|██████████| 29/29 [00:11<00:00,  2.55it/s, v_num=3.41e+7, train_loss_step=0.0946, val_loss=0.193, train_loss_epoch=0.256]\n",
      "Epoch 17: 100%|██████████| 29/29 [00:11<00:00,  2.50it/s, v_num=3.41e+7, train_loss_step=0.0946, val_loss=0.193, train_loss_epoch=0.238]\n",
      "Epoch 18:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.0946, val_loss=0.193, train_loss_epoch=0.238]         \n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:01<00:00,  3.69it/s]\u001b[A\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Epoch 18:  59%|█████▊    | 17/29 [00:05<00:04,  2.92it/s, v_num=3.41e+7, train_loss_step=0.140, val_loss=0.193, train_loss_epoch=0.238]\u001b[32m [repeated 26x across cluster]\u001b[0m\n",
      "Epoch 16:  93%|█████████▎| 27/29 [00:10<00:00,  2.55it/s, v_num=3.41e+7, train_loss_step=0.238, val_loss=0.238, train_loss_epoch=0.281]\n",
      "Epoch 16:  97%|█████████▋| 28/29 [00:11<00:00,  2.54it/s, v_num=3.41e+7, train_loss_step=0.220, val_loss=0.238, train_loss_epoch=0.281]\n",
      "Epoch 16: 100%|██████████| 29/29 [00:11<00:00,  2.60it/s, v_num=3.41e+7, train_loss_step=0.681, val_loss=0.238, train_loss_epoch=0.281]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:01,  3.59it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:01,  3.20it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:00<00:00,  3.14it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:01<00:00,  3.10it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:01<00:00,  3.05it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  3.23it/s]\u001b[A\n",
      "Epoch 16: 100%|██████████| 29/29 [00:13<00:00,  2.22it/s, v_num=3.41e+7, train_loss_step=0.681, val_loss=0.212, train_loss_epoch=0.281]\n",
      "Epoch 16: 100%|██████████| 29/29 [00:13<00:00,  2.18it/s, v_num=3.41e+7, train_loss_step=0.681, val_loss=0.212, train_loss_epoch=0.272]\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Epoch 17:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.681, val_loss=0.212, train_loss_epoch=0.272]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/60537ef6/checkpoint_000016)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Epoch 17:   3%|▎         | 1/29 [00:00<00:11,  2.38it/s, v_num=3.41e+7, train_loss_step=0.206, val_loss=0.212, train_loss_epoch=0.272]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 22:40:54,483\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Epoch 18: 100%|██████████| 29/29 [00:09<00:00,  2.91it/s, v_num=3.41e+7, train_loss_step=0.163, val_loss=0.193, train_loss_epoch=0.238]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:01<00:00,  3.73it/s]\u001b[A\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  3.93it/s]\u001b[A\n",
      "Epoch 18: 100%|██████████| 29/29 [00:11<00:00,  2.51it/s, v_num=3.41e+7, train_loss_step=0.163, val_loss=0.220, train_loss_epoch=0.238]\n",
      "Epoch 18: 100%|██████████| 29/29 [00:11<00:00,  2.46it/s, v_num=3.41e+7, train_loss_step=0.163, val_loss=0.220, train_loss_epoch=0.234]\n",
      "Epoch 19:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.163, val_loss=0.220, train_loss_epoch=0.234]         \n",
      "Epoch 17:  48%|████▊     | 14/29 [00:05<00:06,  2.39it/s, v_num=3.41e+7, train_loss_step=0.258, val_loss=0.212, train_loss_epoch=0.272]\u001b[32m [repeated 23x across cluster]\u001b[0m\n",
      "Epoch 17:  83%|████████▎ | 24/29 [00:09<00:02,  2.47it/s, v_num=3.41e+7, train_loss_step=0.191, val_loss=0.212, train_loss_epoch=0.272]\n",
      "Epoch 17:  83%|████████▎ | 24/29 [00:09<00:02,  2.47it/s, v_num=3.41e+7, train_loss_step=0.134, val_loss=0.212, train_loss_epoch=0.272]\n",
      "Epoch 17:  93%|█████████▎| 27/29 [00:10<00:00,  2.54it/s, v_num=3.41e+7, train_loss_step=0.286, val_loss=0.212, train_loss_epoch=0.272]\n",
      "Epoch 17:  97%|█████████▋| 28/29 [00:10<00:00,  2.56it/s, v_num=3.41e+7, train_loss_step=0.188, val_loss=0.212, train_loss_epoch=0.272]\n",
      "Epoch 19:  90%|████████▉ | 26/29 [00:08<00:01,  2.95it/s, v_num=3.41e+7, train_loss_step=0.249, val_loss=0.220, train_loss_epoch=0.234]\u001b[32m [repeated 27x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00,  5.25it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00,  4.74it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 22:41:05,269\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  4.75it/s]\u001b[A\n",
      "Epoch 17: 100%|██████████| 29/29 [00:12<00:00,  2.35it/s, v_num=3.41e+7, train_loss_step=0.131, val_loss=0.213, train_loss_epoch=0.272]\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/60537ef6/checkpoint_000017)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2025-03-11 22:41:05,497\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 29/29 [00:12<00:00,  2.31it/s, v_num=3.41e+7, train_loss_step=0.131, val_loss=0.213, train_loss_epoch=0.251]\n",
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m \n",
      "Epoch 18:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.131, val_loss=0.213, train_loss_epoch=0.251]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60724)\u001b[0m `Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m *** SIGSEGV received at time=1741729266 on cpu 27 ***\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m PC: @     0x7f9307fb78fe  (unknown)  ray::gcs::TaskInfoAccessor::AsyncAddTaskEventData()\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m     @     0x7f930923e6f0       3680  (unknown)\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m     @     0x7f9307e98ab5       1392  ray::core::worker::TaskEventBufferImpl::FlushEvents()\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m     @     0x7f9307e2182c       1488  ray::core::CoreWorker::Disconnect()\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m     @     0x7f9307e21bdd       1152  ray::core::CoreWorker::ForceExit()\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m     @     0x7f9307e2200f       1680  ray::core::CoreWorker::HandleKillActor()\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m     @     0x7f9307e19514        192  ray::rpc::ServerCallImpl<>::HandleRequestImpl()\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m     @     0x7f93081a6d08       1168  EventTracker::RecordExecution()\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m     @     0x7f930818b13e         48  std::_Function_handler<>::_M_invoke()\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m     @     0x7f930818b5b6        112  boost::asio::detail::completion_handler<>::do_complete()\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m     @     0x7f9308851a5b        128  boost::asio::detail::scheduler::do_run_one()\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m     @     0x7f93088533d9        288  boost::asio::detail::scheduler::run()\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m     @     0x7f9308853ae2         96  boost::asio::io_context::run()\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m     @     0x7f9307d7a1f1       1280  ray::core::CoreWorker::RunIOService()\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m     @     0x7f9308298620         64  thread_proxy\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m     @     0x7f9309289c02  (unknown)  start_thread\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m [2025-03-11 22:41:07,224 E 60487 60618] logging.cc:484: *** SIGSEGV received at time=1741729267 on cpu 27 ***\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m [2025-03-11 22:41:07,224 E 60487 60618] logging.cc:484: PC: @     0x7f9307fb78fe  (unknown)  ray::gcs::TaskInfoAccessor::AsyncAddTaskEventData()\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m [2025-03-11 22:41:07,224 E 60487 60618] logging.cc:484:     @     0x7f930923e6f0       3680  (unknown)\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m [2025-03-11 22:41:07,224 E 60487 60618] logging.cc:484:     @     0x7f9307e98ab5       1392  ray::core::worker::TaskEventBufferImpl::FlushEvents()\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m [2025-03-11 22:41:07,224 E 60487 60618] logging.cc:484:     @     0x7f9307e2182c       1488  ray::core::CoreWorker::Disconnect()\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m [2025-03-11 22:41:07,224 E 60487 60618] logging.cc:484:     @     0x7f9307e21bdd       1152  ray::core::CoreWorker::ForceExit()\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m [2025-03-11 22:41:07,224 E 60487 60618] logging.cc:484:     @     0x7f9307e2200f       1680  ray::core::CoreWorker::HandleKillActor()\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m [2025-03-11 22:41:07,224 E 60487 60618] logging.cc:484:     @     0x7f9307e19514        192  ray::rpc::ServerCallImpl<>::HandleRequestImpl()\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m [2025-03-11 22:41:07,224 E 60487 60618] logging.cc:484:     @     0x7f93081a6d08       1168  EventTracker::RecordExecution()\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m [2025-03-11 22:41:07,224 E 60487 60618] logging.cc:484:     @     0x7f930818b13e         48  std::_Function_handler<>::_M_invoke()\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m [2025-03-11 22:41:07,224 E 60487 60618] logging.cc:484:     @     0x7f930818b5b6        112  boost::asio::detail::completion_handler<>::do_complete()\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m [2025-03-11 22:41:07,224 E 60487 60618] logging.cc:484:     @     0x7f9308851a5b        128  boost::asio::detail::scheduler::do_run_one()\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m [2025-03-11 22:41:07,224 E 60487 60618] logging.cc:484:     @     0x7f93088533d9        288  boost::asio::detail::scheduler::run()\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m [2025-03-11 22:41:07,224 E 60487 60618] logging.cc:484:     @     0x7f9308853ae2         96  boost::asio::io_context::run()\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m [2025-03-11 22:41:07,224 E 60487 60618] logging.cc:484:     @     0x7f9307d7a1f1       1280  ray::core::CoreWorker::RunIOService()\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m [2025-03-11 22:41:07,224 E 60487 60618] logging.cc:484:     @     0x7f9308298620         64  thread_proxy\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m [2025-03-11 22:41:07,224 E 60487 60618] logging.cc:484:     @     0x7f9309289c02  (unknown)  start_thread\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m Fatal Python error: Segmentation fault\n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m \n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m \n",
      "\u001b[36m(TorchTrainer pid=60487)\u001b[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, simplejson._speedups, ray._raylet, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pyarrow.lib, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, pyarrow._parquet, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, gmpy2.gmpy2, markupsafe._speedups, PIL._imaging, pyarrow._json, rdkit.rdBase, rdkit.DataStructs.cDataStructs, rdkit.Chem.rdchem, rdkit.Geometry.rdGeometry, rdkit.Chem.rdinchi, rdkit.Chem.rdCIPLabeler, rdkit.Chem.rdmolfiles, rdkit.Chem.rdmolops, rdkit.Chem.rdMolInterchange, rdkit.Chem.rdCoordGen, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._highs.cython.src._highs_wrapper, scipy.optimize._highs._highs_wrapper, scipy.optimize._highs.cython.src._highs_constants, scipy.optimize._highs._highs_constants, scipy.linalg._interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._bspl, scipy.interpolate._ppoly, scipy.interpolate.interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._levy_stable.levyst, scipy.stats._stats_pythran, scipy._lib._uarray._uarray, scipy.stats._ansari_swilk_statistics, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.stats._unuran.unuran_wrapper, scipy.ndimage._nd_image, _ni_label, scipy.ndimage._ni_label, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, rdkit.ForceField.rdForceField, rdkit.Chem.rdChemicalFeatures, rdkit.Chem.rdMolChemicalFeatures, rdkit.Chem.rdDistGeom, rdkit.Chem.rdChemReactions, rdkit.Chem.rdDepictor, rdkit.Chem.rdFingerprintGenerator, rdkit.Chem.rdForceFieldHelpers, rdkit.Chem.rdMolAlign, rdkit.Chem.rdMolDescriptors, rdkit.Chem.rdMolEnumerator, rdkit.Chem.rdMolTransforms, rdkit.Chem.rdPartialCharges, rdkit.Chem.rdqueries, rdkit.Chem.rdReducedGraphs, rdkit.Chem.rdShapeHelpers, rdkit.Chem.rdSLNParse, rdkit.ML.InfoTheory.rdInfoTheory, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, sklearn.utils._fast_dict, sklearn.cluster._hierarchical_fast, sklearn.cluster._k_means_common, sklearn.cluster._k_means_elkan, sklearn.cluster._k_means_lloyd, sklearn.cluster._k_means_minibatch, sklearn.neighbors._partition_nodes, sklearn.neighbors._ball_tree, sklearn.neighbors._kd_tree, sklearn.utils.arrayfuncs, sklearn.utils._random, sklearn.utils._seq_dataset, sklearn.linear_model._cd_fast, _loss, sklearn._loss._loss, sklearn.svm._liblinear, sklearn.svm._libsvm, sklearn.svm._libsvm_sparse, sklearn.linear_model._sag_fast, sklearn.utils._weight_vector, sklearn.linear_model._sgd_fast, sklearn.decomposition._online_lda_fast, sklearn.decomposition._cdnmf_fast, sklearn.cluster._dbscan_inner, sklearn.cluster._hdbscan._tree, sklearn.cluster._hdbscan._linkage, sklearn.cluster._hdbscan._reachability, sklearn._isotonic, sklearn.tree._utils, sklearn.tree._tree, sklearn.tree._partitioner, sklearn.tree._splitter, sklearn.tree._criterion, sklearn.neighbors._quad_tree, sklearn.manifold._barnes_hut_tsne, sklearn.manifold._utils, rdkit.ML.Cluster.Clustering, rdkit.Avalon.pyAvalonTools, rdkit.Chem.Draw.rdMolDraw2D, kiwisolver._cext, scipy.cluster._vq, scipy.cluster._hierarchy, scipy.cluster._optimal_leaf_ordering, sklearn.ensemble._gradient_boosting, sklearn.ensemble._hist_gradient_boosting.common, sklearn.ensemble._hist_gradient_boosting._gradient_boosting, sklearn.ensemble._hist_gradient_boosting._binning, sklearn.ensemble._hist_gradient_boosting._bitset, sklearn.ensemble._hist_gradient_boosting.histogram, sklearn.ensemble._hist_gradient_boosting._predictor, sklearn.ensemble._hist_gradient_boosting.splitting, scipy.signal._sigtools, scipy.signal._max_len_seq_inner, scipy.signal._upfirdn_apply, scipy.signal._spline, scipy.signal._sosfilt, scipy.signal._spectral, scipy.signal._peak_finding_utils, regex._regex (total: 273)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 29/29 [00:09<00:00,  3.07it/s, v_num=3.41e+7, train_loss_step=0.120, val_loss=0.220, train_loss_epoch=0.234]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Epoch 18:  38%|███▊      | 11/29 [00:03<00:05,  3.31it/s, v_num=3.41e+7, train_loss_step=0.238, val_loss=0.213, train_loss_epoch=0.251]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00<00:00,  5.29it/s]\u001b[A\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  5.29it/s]\u001b[A\n",
      "Epoch 19: 100%|██████████| 29/29 [00:10<00:00,  2.73it/s, v_num=3.41e+7, train_loss_step=0.120, val_loss=0.181, train_loss_epoch=0.234]\n",
      "Epoch 19: 100%|██████████| 29/29 [00:10<00:00,  2.65it/s, v_num=3.41e+7, train_loss_step=0.120, val_loss=0.181, train_loss_epoch=0.217]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 18:  97%|█████████▋| 28/29 [00:08<00:00,  3.43it/s, v_num=3.41e+7, train_loss_step=0.282, val_loss=0.213, train_loss_epoch=0.251]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Epoch 18:  90%|████████▉ | 26/29 [00:07<00:00,  3.41it/s, v_num=3.41e+7, train_loss_step=0.276, val_loss=0.213, train_loss_epoch=0.251]\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:00,  5.18it/s]\u001b[A\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  5.24it/s]\u001b[A\n",
      "Epoch 18: 100%|██████████| 29/29 [00:09<00:00,  3.07it/s, v_num=3.41e+7, train_loss_step=0.150, val_loss=0.203, train_loss_epoch=0.251]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/60537ef6/checkpoint_000018)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19:   0%|          | 0/29 [00:00<?, ?it/s, v_num=3.41e+7, train_loss_step=0.150, val_loss=0.203, train_loss_epoch=0.256]         \n",
      "Epoch 18: 100%|██████████| 29/29 [00:09<00:00,  3.02it/s, v_num=3.41e+7, train_loss_step=0.150, val_loss=0.203, train_loss_epoch=0.256]\n",
      "Epoch 18: 100%|██████████| 29/29 [00:08<00:00,  3.51it/s, v_num=3.41e+7, train_loss_step=0.150, val_loss=0.213, train_loss_epoch=0.251]\n",
      "Epoch 19:  48%|████▊     | 14/29 [00:03<00:04,  3.59it/s, v_num=3.41e+7, train_loss_step=0.246, val_loss=0.203, train_loss_epoch=0.256]\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:01<00:00,  4.96it/s]\u001b[A\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Epoch 19:  93%|█████████▎| 27/29 [00:07<00:00,  3.60it/s, v_num=3.41e+7, train_loss_step=0.233, val_loss=0.203, train_loss_epoch=0.256]\n",
      "Epoch 19:  97%|█████████▋| 28/29 [00:07<00:00,  3.61it/s, v_num=3.41e+7, train_loss_step=0.294, val_loss=0.203, train_loss_epoch=0.256]\n",
      "Epoch 19: 100%|██████████| 29/29 [00:07<00:00,  3.70it/s, v_num=3.41e+7, train_loss_step=0.159, val_loss=0.203, train_loss_epoch=0.256]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m \n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:01<00:00,  5.22it/s]\u001b[A\n",
      "Epoch 19: 100%|██████████| 29/29 [00:09<00:00,  3.21it/s, v_num=3.41e+7, train_loss_step=0.159, val_loss=0.196, train_loss_epoch=0.256]\n",
      "Epoch 19:  90%|████████▉ | 26/29 [00:07<00:00,  3.60it/s, v_num=3.41e+7, train_loss_step=0.253, val_loss=0.203, train_loss_epoch=0.256]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "Epoch 19: 100%|██████████| 29/29 [00:09<00:00,  3.15it/s, v_num=3.41e+7, train_loss_step=0.159, val_loss=0.196, train_loss_epoch=0.239]\n",
      "Epoch 19: 100%|██████████| 29/29 [00:09<00:00,  3.12it/s, v_num=3.41e+7, train_loss_step=0.159, val_loss=0.196, train_loss_epoch=0.239]\n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:01<00:00,  4.94it/s]\u001b[A\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/60537ef6/checkpoint_000019)\n",
      "\u001b[36m(RayTrainWorker pid=60488)\u001b[0m `Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "2025-03-11 22:41:25,587\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53' in 0.0065s.\n",
      "2025-03-11 22:41:25,592\tINFO tune.py:1041 -- Total run time: 271.73 seconds (271.63 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "ray.init()\n",
    "\n",
    "scheduler = FIFOScheduler()\n",
    "\n",
    "# Scaling config controls the resources used by Ray\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=1,\n",
    "    use_gpu=False, # change to True if you want to use GPU\n",
    ")\n",
    "\n",
    "# Checkpoint config controls the checkpointing behavior of Ray\n",
    "checkpoint_config = CheckpointConfig(\n",
    "    num_to_keep=1, # number of checkpoints to keep\n",
    "    checkpoint_score_attribute=\"val_loss\", # Save the checkpoint based on this metric\n",
    "    checkpoint_score_order=\"min\", # Save the checkpoint with the lowest metric value\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    checkpoint_config=checkpoint_config,\n",
    "    storage_path=hpopt_save_dir / \"ray_results\", # directory to save the results\n",
    ")\n",
    "\n",
    "ray_trainer = TorchTrainer(\n",
    "    lambda config: train_model(\n",
    "        config, train_dset, val_dset, num_workers, scaler\n",
    "    ),\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    ")\n",
    "\n",
    "search_alg = HyperOptSearch(\n",
    "    n_initial_points=1, # number of random evaluations before tree parzen estimators\n",
    "    random_state_seed=42,\n",
    ")\n",
    "\n",
    "# OptunaSearch is another search algorithm that can be used\n",
    "# search_alg = OptunaSearch()\n",
    "\n",
    "tune_config = tune.TuneConfig(\n",
    "    metric=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    num_samples=2, # number of trials to run\n",
    "    scheduler=scheduler,\n",
    "    search_alg=search_alg,\n",
    "    trial_dirname_creator=lambda trial: str(trial.trial_id), # shorten filepaths\n",
    "\n",
    ")\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    ray_trainer,\n",
    "    param_space={\n",
    "        \"train_loop_config\": search_space,\n",
    "    },\n",
    "    tune_config=tune_config,\n",
    ")\n",
    "\n",
    "# Start the hyperparameter search\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58abd66a-7791-41d5-b392-53fce5167960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResultGrid<[\n",
       "  Result(\n",
       "    metrics={'train_loss': 0.2392559051513672, 'train_loss_step': 0.15869145095348358, 'val/rmse': 0.4428632855415344, 'val/mae': 0.34596309065818787, 'val_loss': 0.19612789154052734, 'train_loss_epoch': 0.2392559051513672, 'epoch': 19, 'step': 580},\n",
       "    path='/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/60537ef6',\n",
       "    filesystem='local',\n",
       "    checkpoint=Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/60537ef6/checkpoint_000019)\n",
       "  ),\n",
       "  Result(\n",
       "    metrics={'train_loss': 0.21735358238220215, 'train_loss_step': 0.11986755579710007, 'val/rmse': 0.4251277446746826, 'val/mae': 0.3320632874965668, 'val_loss': 0.1807335913181305, 'train_loss_epoch': 0.21735358238220215, 'epoch': 19, 'step': 580},\n",
       "    path='/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/73171dbb',\n",
       "    filesystem='local',\n",
       "    checkpoint=Checkpoint(filesystem=local, path=/ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/73171dbb/checkpoint_000019)\n",
       "  )\n",
       "]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3936aab-2566-49a1-88a7-0f0425665d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_loss_step</th>\n",
       "      <th>val/rmse</th>\n",
       "      <th>val/mae</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>train_loss_epoch</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>checkpoint_dir_name</th>\n",
       "      <th>...</th>\n",
       "      <th>pid</th>\n",
       "      <th>hostname</th>\n",
       "      <th>node_ip</th>\n",
       "      <th>time_since_restore</th>\n",
       "      <th>iterations_since_restore</th>\n",
       "      <th>config/train_loop_config/depth</th>\n",
       "      <th>config/train_loop_config/ffn_hidden_dim</th>\n",
       "      <th>config/train_loop_config/ffn_num_layers</th>\n",
       "      <th>config/train_loop_config/message_hidden_dim</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.239256</td>\n",
       "      <td>0.158691</td>\n",
       "      <td>0.442863</td>\n",
       "      <td>0.345963</td>\n",
       "      <td>0.196128</td>\n",
       "      <td>0.239256</td>\n",
       "      <td>19</td>\n",
       "      <td>580</td>\n",
       "      <td>1741729284</td>\n",
       "      <td>checkpoint_000019</td>\n",
       "      <td>...</td>\n",
       "      <td>60246</td>\n",
       "      <td>cpusrv27.scidom.de</td>\n",
       "      <td>10.233.0.37</td>\n",
       "      <td>260.430448</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>60537ef6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.217354</td>\n",
       "      <td>0.119868</td>\n",
       "      <td>0.425128</td>\n",
       "      <td>0.332063</td>\n",
       "      <td>0.180734</td>\n",
       "      <td>0.217354</td>\n",
       "      <td>19</td>\n",
       "      <td>580</td>\n",
       "      <td>1741729265</td>\n",
       "      <td>checkpoint_000019</td>\n",
       "      <td>...</td>\n",
       "      <td>60487</td>\n",
       "      <td>cpusrv27.scidom.de</td>\n",
       "      <td>10.233.0.37</td>\n",
       "      <td>232.148599</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>2200</td>\n",
       "      <td>2</td>\n",
       "      <td>400</td>\n",
       "      <td>73171dbb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_loss  train_loss_step  val/rmse   val/mae  val_loss  \\\n",
       "0    0.239256         0.158691  0.442863  0.345963  0.196128   \n",
       "1    0.217354         0.119868  0.425128  0.332063  0.180734   \n",
       "\n",
       "   train_loss_epoch  epoch  step   timestamp checkpoint_dir_name  ...    pid  \\\n",
       "0          0.239256     19   580  1741729284   checkpoint_000019  ...  60246   \n",
       "1          0.217354     19   580  1741729265   checkpoint_000019  ...  60487   \n",
       "\n",
       "             hostname      node_ip time_since_restore  \\\n",
       "0  cpusrv27.scidom.de  10.233.0.37         260.430448   \n",
       "1  cpusrv27.scidom.de  10.233.0.37         232.148599   \n",
       "\n",
       "  iterations_since_restore  config/train_loop_config/depth  \\\n",
       "0                       20                               2   \n",
       "1                       20                               2   \n",
       "\n",
       "   config/train_loop_config/ffn_hidden_dim  \\\n",
       "0                                     2000   \n",
       "1                                     2200   \n",
       "\n",
       "   config/train_loop_config/ffn_num_layers  \\\n",
       "0                                        2   \n",
       "1                                        2   \n",
       "\n",
       "  config/train_loop_config/message_hidden_dim    logdir  \n",
       "0                                         500  60537ef6  \n",
       "1                                         400  73171dbb  \n",
       "\n",
       "[2 rows x 27 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = results.get_dataframe()\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e8dd786-358c-46cd-b69c-9e6b1f2343aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'depth': 2,\n",
       " 'ffn_hidden_dim': 2200,\n",
       " 'ffn_num_layers': 2,\n",
       " 'message_hidden_dim': 400}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best configuration\n",
    "best_result = results.get_best_result()\n",
    "best_config = best_result.config\n",
    "best_config['train_loop_config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce9c0344-f3d8-463f-ab5e-6612385cf99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model checkpoint path: /ictstr01/home/aih/serra.korkmaz/projects/saturn/hopt/hpopt_braf_chembl/ray_results/TorchTrainer_2025-03-11_22-36-53/73171dbb/checkpoint_000019/checkpoint.ckpt\n"
     ]
    }
   ],
   "source": [
    "# best model checkpoint path\n",
    "best_result = results.get_best_result()\n",
    "best_checkpoint_path = Path(best_result.checkpoint.path) / \"checkpoint.ckpt\"\n",
    "print(f\"Best model checkpoint path: {best_checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c8d7370-2604-4965-93ad-ea8bdeb1eca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNN(\n",
       "  (message_passing): BondMessagePassing(\n",
       "    (W_i): Linear(in_features=86, out_features=400, bias=False)\n",
       "    (W_h): Linear(in_features=400, out_features=400, bias=False)\n",
       "    (W_o): Linear(in_features=472, out_features=400, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (tau): ReLU()\n",
       "    (V_d_transform): Identity()\n",
       "    (graph_transform): Identity()\n",
       "  )\n",
       "  (agg): MeanAggregation()\n",
       "  (bn): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (predictor): RegressionFFN(\n",
       "    (ffn): MLP(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=400, out_features=2200, bias=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Linear(in_features=2200, out_features=2200, bias=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Linear(in_features=2200, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (criterion): MSE(task_weights=[[1.0]])\n",
       "    (output_transform): UnscaleTransform()\n",
       "  )\n",
       "  (X_d_transform): Identity()\n",
       "  (metrics): ModuleList(\n",
       "    (0): RMSE(task_weights=[[1.0]])\n",
       "    (1): MAE(task_weights=[[1.0]])\n",
       "    (2): MSE(task_weights=[[1.0]])\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpnn = models.MPNN.load_from_checkpoint(best_checkpoint_path)\n",
    "mpnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a8a34e4-ca47-44b9-8cf3-7f6ac5b152ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/aih/serra.korkmaz/miniconda3/envs/saturn/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n",
      "/home/aih/serra.korkmaz/miniconda3/envs/saturn/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 4/4 [00:00<00:00, 10.63it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "test_loader = data.build_dataloader(test_dset, shuffle=False)\n",
    "with torch.inference_mode():\n",
    "    trainer = pl.Trainer(\n",
    "        logger=None,\n",
    "        enable_progress_bar=True,\n",
    "        accelerator=\"cpu\",\n",
    "        devices=1\n",
    "    )\n",
    "    test_preds = trainer.predict(mpnn, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69b84a2f-36df-415a-af1c-f7595b59b7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>value</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O=C(Nc1ccc(F)cc1)C(c1ccccc1)N1CCN(c2ccncc2)CC1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.348618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CN1CCN(c2ccc(N=c3c4c([nH]c5cc(Cl)ccc35)CCCC4)c...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.540173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cc1ccccc1C(CC(=O)O)NC(=O)c1cccc(-c2ccc(F)cc2)n1</td>\n",
       "      <td>9.6</td>\n",
       "      <td>9.721862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N=c1ncc(-c2nc(N3CCOCC3)nc3c2CCN3C2CCN(C=O)C2)c...</td>\n",
       "      <td>9.6</td>\n",
       "      <td>9.340010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CC1Cc2c([nH]c3ccc(F)cc23)C2(CCC(c3ccccc3)(N(C)...</td>\n",
       "      <td>9.2</td>\n",
       "      <td>7.948752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>COC(=O)NC(C(=O)NC(Cc1ccccc1)C(O)CN(OC1CCCC1)S(...</td>\n",
       "      <td>8.4</td>\n",
       "      <td>8.060696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Cc1cc2c(cc1Cl)SC(C(=O)c1ccc(Br)cc1)=NS2(=O)=O</td>\n",
       "      <td>8.1</td>\n",
       "      <td>8.355878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>COc1ccc(C(=O)OC(C=C(C)C)CC(C)C2CCC3(C)C4C(OC)C...</td>\n",
       "      <td>9.4</td>\n",
       "      <td>7.956693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Cc1nc2ccccc2nc1N1CC2CN(C(=O)c3ccccc3-c3nc[nH]n...</td>\n",
       "      <td>9.9</td>\n",
       "      <td>9.335267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>CC1COc2ccccc2N1C(=O)c1cscn1</td>\n",
       "      <td>7.6</td>\n",
       "      <td>7.059649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                smiles  value     preds\n",
       "0       O=C(Nc1ccc(F)cc1)C(c1ccccc1)N1CCN(c2ccncc2)CC1    9.0  8.348618\n",
       "1    CN1CCN(c2ccc(N=c3c4c([nH]c5cc(Cl)ccc35)CCCC4)c...   10.0  9.540173\n",
       "2      Cc1ccccc1C(CC(=O)O)NC(=O)c1cccc(-c2ccc(F)cc2)n1    9.6  9.721862\n",
       "3    N=c1ncc(-c2nc(N3CCOCC3)nc3c2CCN3C2CCN(C=O)C2)c...    9.6  9.340010\n",
       "4    CC1Cc2c([nH]c3ccc(F)cc23)C2(CCC(c3ccccc3)(N(C)...    9.2  7.948752\n",
       "..                                                 ...    ...       ...\n",
       "195  COC(=O)NC(C(=O)NC(Cc1ccccc1)C(O)CN(OC1CCCC1)S(...    8.4  8.060696\n",
       "196      Cc1cc2c(cc1Cl)SC(C(=O)c1ccc(Br)cc1)=NS2(=O)=O    8.1  8.355878\n",
       "197  COc1ccc(C(=O)OC(C=C(C)C)CC(C)C2CCC3(C)C4C(OC)C...    9.4  7.956693\n",
       "198  Cc1nc2ccccc2nc1N1CC2CN(C(=O)c3ccccc3-c3nc[nH]n...    9.9  9.335267\n",
       "199                        CC1COc2ccccc2N1C(=O)c1cscn1    7.6  7.059649\n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_preds = np.concatenate(test_preds, axis=0)\n",
    "df_test['preds'] = test_preds\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de73bcf-ab98-4b62-a767-33f347950575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02182757-35e6-4af4-95cb-5dfc58ce8afc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
